{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report\n",
    "## Swin Transformer Performance on PlantVillage Test Set\n",
    "\n",
    "**Metrics**: Accuracy, F1, Precision, Recall, AUROC, mAP@0.5, Confusion Matrix, Per-Class ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from src.visualization.evaluation_plots import plot_confusion_matrix, plot_roc_curves, plot_error_gallery\n",
    "from src.visualization.training_plots import plot_metric_comparison\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = Path('../../reports/metrics/test_metrics.json')\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path) as f:\n",
    "        metrics = json.load(f)\n",
    "\n",
    "    print('=== Overall Metrics ===')\n",
    "    for k, v in metrics['overall'].items():\n",
    "        print(f'  {k}: {v:.4f}' if isinstance(v, float) else f'  {k}: {v}')\n",
    "\n",
    "    print('\\n=== Per-Class F1 ===')\n",
    "    f1_scores = {}\n",
    "    for cls_name, cls_metrics in metrics.get('per_class', {}).items():\n",
    "        f1 = cls_metrics.get('f1', 0)\n",
    "        f1_scores[cls_name] = f1\n",
    "        print(f'  {cls_name}: {f1:.4f}')\n",
    "\n",
    "    if f1_scores:\n",
    "        plot_metric_comparison(f1_scores, title='Per-Class F1 Score')\n",
    "else:\n",
    "    print('No metrics file found. Run evaluate.py first.')\n",
    "    print('Command: python evaluate.py --config configs/config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_path = Path('../../reports/metrics/confusion_matrix.png')\n",
    "if cm_path.exists():\n",
    "    img = Image.open(cm_path)\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    plt.title('Confusion Matrix (Normalized)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Confusion matrix plot not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_path = Path('../../reports/metrics/roc_curves.png')\n",
    "if roc_path.exists():\n",
    "    img = Image.open(roc_path)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    plt.title('ROC Curves (One-vs-Rest)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('ROC curves plot not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_path = Path('../../reports/metrics/error_gallery.png')\n",
    "if error_path.exists():\n",
    "    img = Image.open(error_path)\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    plt.title('Misclassified Samples')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Error gallery not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Targets Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics_path.exists():\n",
    "    targets = {\n",
    "        'mAP@0.5': (metrics['overall'].get('mAP@0.5', 0), 0.85),\n",
    "        'F1-Score': (metrics['overall'].get('f1_macro', 0), 0.90),\n",
    "        'Precision': (metrics['overall'].get('precision_macro', 0), 0.92),\n",
    "    }\n",
    "\n",
    "    print('Performance Target Check:')\n",
    "    for name, (actual, target) in targets.items():\n",
    "        status = 'PASS' if actual >= target else 'FAIL'\n",
    "        print(f'  {name}: {actual:.4f} (target: {target}) [{status}]')\n",
    "else:\n",
    "    print('Run evaluation first.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
